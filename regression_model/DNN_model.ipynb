{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from config import *\n",
    "import torch.nn as nn\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "\n",
    "train_data_path = r'./DNN_input/all_data.csv'\n",
    "train = pd.read_csv(train_data_path)\n",
    "\n",
    "test_data_path = r'./DNN_input/yf_test.csv'\n",
    "test = pd.read_csv(test_data_path)\n",
    "\n",
    "np.random.seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "data": {
      "text/plain": "(99, 62)"
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     5.95492\n",
      "1     4.87705\n",
      "2     4.53279\n",
      "3     4.97951\n",
      "4     6.12295\n",
      "       ...   \n",
      "94    2.40000\n",
      "95    7.54000\n",
      "96    6.50000\n",
      "97    6.00000\n",
      "98    7.30000\n",
      "Name: score, Length: 99, dtype: float64\n",
      "     leq_mean   leq_std     leq_25  leq_median     leq_75  leq_10-leq_90  \\\n",
      "0   64.666450  1.565390  63.354370   64.548130  65.572620       4.143830   \n",
      "1   64.468650  2.127060  63.427070   64.530420  65.955460       5.367400   \n",
      "2   62.650530  2.718450  60.374090   62.206400  64.263670       7.005440   \n",
      "3   64.140550  2.676680  62.631000   63.858930  65.689140       7.079710   \n",
      "4   64.211750  2.655630  61.744950   63.997690  66.869360       6.542660   \n",
      "..        ...       ...        ...         ...        ...            ...   \n",
      "94  51.990823  2.270476  50.202316   51.807796  53.696781       5.727438   \n",
      "95  52.507340  1.181550  51.910330   52.550910  53.204060       2.762190   \n",
      "96  52.512260  0.818230  52.072780   52.357920  52.694810       1.919300   \n",
      "97  52.953050  0.851420  52.527070   52.763920  53.070960       1.355290   \n",
      "98  53.598800  1.040900  53.062010   53.397650  53.854580       1.684290   \n",
      "\n",
      "    loudness_mean  loudness_std  loudness_25  loudness_median  ...  fluct_25  \\\n",
      "0       17.754900      1.869170      16.2550          17.2850  ...  0.007830   \n",
      "1       15.933690      2.017870      14.9885          16.1050  ...  0.006800   \n",
      "2       14.615140      2.427800      12.6235          14.1230  ...  0.005830   \n",
      "3       14.421970      2.368710      12.9970          14.0360  ...  0.056990   \n",
      "4       16.293150      3.114290      13.5190          15.6545  ...  0.008470   \n",
      "..            ...           ...          ...              ...  ...       ...   \n",
      "94       6.799631      1.136502       5.9390           6.5580  ...  0.004458   \n",
      "95       7.825430      0.769260       7.3650           7.7980  ...  0.002970   \n",
      "96       7.805940      0.571590       7.4950           7.7270  ...  0.002260   \n",
      "97       8.033220      0.675460       7.7390           7.9230  ...  0.002440   \n",
      "98       8.378580      0.867160       7.9100           8.2310  ...  0.003040   \n",
      "\n",
      "    fluct_median  fluct_75  fluct_10-fluct_90  tonality_mean  tonality_std  \\\n",
      "0       0.014730  0.019840           0.062710        0.05675      0.041700   \n",
      "1       0.009860  0.015060           0.062900        0.07857      0.038600   \n",
      "2       0.007350  0.036990           0.075510        0.03378      0.029600   \n",
      "3       0.067860  0.077990           0.043130        0.07578      0.054350   \n",
      "4       0.010290  0.023100           0.066980        0.03665      0.051470   \n",
      "..           ...       ...                ...            ...           ...   \n",
      "94      0.006561  0.010576           0.012764        0.06389      0.051854   \n",
      "95      0.003750  0.006050           0.006320        0.02897      0.030960   \n",
      "96      0.002780  0.003430           0.002240        0.03030      0.032600   \n",
      "97      0.002960  0.003620           0.003080        0.02914      0.031000   \n",
      "98      0.003800  0.005860           0.008040        0.03099      0.033560   \n",
      "\n",
      "    tonality_25  tonality_median  tonality_75  tonality_10-tonality_90  \n",
      "0      0.028230         0.053160     0.080160                 0.110410  \n",
      "1      0.053770         0.071420     0.096460                 0.091310  \n",
      "2      0.000000         0.035440     0.055890                 0.075450  \n",
      "3      0.038250         0.064470     0.111750                 0.142940  \n",
      "4      0.000000         0.027970     0.052820                 0.080910  \n",
      "..          ...              ...          ...                      ...  \n",
      "94     0.028721         0.058621     0.093968                 0.126624  \n",
      "95     0.000000         0.026110     0.049120                 0.072290  \n",
      "96     0.000000         0.025070     0.053730                 0.074330  \n",
      "97     0.000000         0.025720     0.050470                 0.070630  \n",
      "98     0.000000         0.026930     0.051930                 0.075110  \n",
      "\n",
      "[99 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# y 表示平均吵闹度\n",
    "train_y = train.score\n",
    "train_X = train[column_36]\n",
    "test_y = test.score\n",
    "test_X = test[column_36]\n",
    "print(train_y), print(train_X)\n",
    "num_of_train = train.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [],
   "source": [
    "combined = train_X.append(test_X)\n",
    "combined.reset_index(inplace=True)\n",
    "combined.drop(['index'], inplace=True, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [
    {
     "data": {
      "text/plain": "      leq_mean   leq_std     leq_25  leq_median     leq_75  leq_10-leq_90  \\\n0    64.666450  1.565390  63.354370   64.548130  65.572620       4.143830   \n1    64.468650  2.127060  63.427070   64.530420  65.955460       5.367400   \n2    62.650530  2.718450  60.374090   62.206400  64.263670       7.005440   \n3    64.140550  2.676680  62.631000   63.858930  65.689140       7.079710   \n4    64.211750  2.655630  61.744950   63.997690  66.869360       6.542660   \n..         ...       ...        ...         ...        ...            ...   \n169  51.684594  2.950931  49.122250   51.338657  53.338589       7.694008   \n170  56.086409  5.981590  50.723267   56.433147  59.950764      16.290859   \n171  50.047239  2.401969  48.324600   49.654362  51.642471       6.221943   \n172  50.874526  2.089157  49.593983   50.240971  51.657730       4.288685   \n173  51.327230  1.785935  49.961189   51.132454  52.213837       3.964472   \n\n     loudness_mean  loudness_std  loudness_25  loudness_median  ...  fluct_25  \\\n0        17.754900      1.869170      16.2550          17.2850  ...  0.007830   \n1        15.933690      2.017870      14.9885          16.1050  ...  0.006800   \n2        14.615140      2.427800      12.6235          14.1230  ...  0.005830   \n3        14.421970      2.368710      12.9970          14.0360  ...  0.056990   \n4        16.293150      3.114290      13.5190          15.6545  ...  0.008470   \n..             ...           ...          ...              ...  ...       ...   \n169       6.144695      1.181480       5.2240           5.9050  ...  0.004002   \n170       7.688258      2.892483       5.3490           7.2630  ...  0.006362   \n171       5.904786      1.073166       5.2730           5.6860  ...  0.006031   \n172       6.420369      0.943794       5.8540           6.2000  ...  0.003055   \n173       6.673368      0.785199       6.0770           6.5830  ...  0.002669   \n\n     fluct_median  fluct_75  fluct_10-fluct_90  tonality_mean  tonality_std  \\\n0        0.014730  0.019840           0.062710       0.056750      0.041700   \n1        0.009860  0.015060           0.062900       0.078570      0.038600   \n2        0.007350  0.036990           0.075510       0.033780      0.029600   \n3        0.067860  0.077990           0.043130       0.075780      0.054350   \n4        0.010290  0.023100           0.066980       0.036650      0.051470   \n..            ...       ...                ...            ...           ...   \n169      0.010360  0.022415           0.032302       0.118365      0.061495   \n170      0.009434  0.012003           0.017800       0.063056      0.048654   \n171      0.014525  0.032093           0.042514       0.040851      0.040539   \n172      0.005694  0.010639           0.031608       0.041429      0.038538   \n173      0.004922  0.014560           0.026338       0.062299      0.050873   \n\n     tonality_25  tonality_median  tonality_75  tonality_10-tonality_90  \n0       0.028230         0.053160     0.080160                 0.110410  \n1       0.053770         0.071420     0.096460                 0.091310  \n2       0.000000         0.035440     0.055890                 0.075450  \n3       0.038250         0.064470     0.111750                 0.142940  \n4       0.000000         0.027970     0.052820                 0.080910  \n..           ...              ...          ...                      ...  \n169     0.077229         0.106951     0.144127                 0.136776  \n170     0.024658         0.057933     0.093155                 0.128203  \n171     0.000000         0.036629     0.067197                 0.097571  \n172     0.000000         0.039179     0.065948                 0.091092  \n173     0.022004         0.057294     0.093313                 0.130287  \n\n[174 rows x 36 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>leq_mean</th>\n      <th>leq_std</th>\n      <th>leq_25</th>\n      <th>leq_median</th>\n      <th>leq_75</th>\n      <th>leq_10-leq_90</th>\n      <th>loudness_mean</th>\n      <th>loudness_std</th>\n      <th>loudness_25</th>\n      <th>loudness_median</th>\n      <th>...</th>\n      <th>fluct_25</th>\n      <th>fluct_median</th>\n      <th>fluct_75</th>\n      <th>fluct_10-fluct_90</th>\n      <th>tonality_mean</th>\n      <th>tonality_std</th>\n      <th>tonality_25</th>\n      <th>tonality_median</th>\n      <th>tonality_75</th>\n      <th>tonality_10-tonality_90</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>64.666450</td>\n      <td>1.565390</td>\n      <td>63.354370</td>\n      <td>64.548130</td>\n      <td>65.572620</td>\n      <td>4.143830</td>\n      <td>17.754900</td>\n      <td>1.869170</td>\n      <td>16.2550</td>\n      <td>17.2850</td>\n      <td>...</td>\n      <td>0.007830</td>\n      <td>0.014730</td>\n      <td>0.019840</td>\n      <td>0.062710</td>\n      <td>0.056750</td>\n      <td>0.041700</td>\n      <td>0.028230</td>\n      <td>0.053160</td>\n      <td>0.080160</td>\n      <td>0.110410</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>64.468650</td>\n      <td>2.127060</td>\n      <td>63.427070</td>\n      <td>64.530420</td>\n      <td>65.955460</td>\n      <td>5.367400</td>\n      <td>15.933690</td>\n      <td>2.017870</td>\n      <td>14.9885</td>\n      <td>16.1050</td>\n      <td>...</td>\n      <td>0.006800</td>\n      <td>0.009860</td>\n      <td>0.015060</td>\n      <td>0.062900</td>\n      <td>0.078570</td>\n      <td>0.038600</td>\n      <td>0.053770</td>\n      <td>0.071420</td>\n      <td>0.096460</td>\n      <td>0.091310</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>62.650530</td>\n      <td>2.718450</td>\n      <td>60.374090</td>\n      <td>62.206400</td>\n      <td>64.263670</td>\n      <td>7.005440</td>\n      <td>14.615140</td>\n      <td>2.427800</td>\n      <td>12.6235</td>\n      <td>14.1230</td>\n      <td>...</td>\n      <td>0.005830</td>\n      <td>0.007350</td>\n      <td>0.036990</td>\n      <td>0.075510</td>\n      <td>0.033780</td>\n      <td>0.029600</td>\n      <td>0.000000</td>\n      <td>0.035440</td>\n      <td>0.055890</td>\n      <td>0.075450</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>64.140550</td>\n      <td>2.676680</td>\n      <td>62.631000</td>\n      <td>63.858930</td>\n      <td>65.689140</td>\n      <td>7.079710</td>\n      <td>14.421970</td>\n      <td>2.368710</td>\n      <td>12.9970</td>\n      <td>14.0360</td>\n      <td>...</td>\n      <td>0.056990</td>\n      <td>0.067860</td>\n      <td>0.077990</td>\n      <td>0.043130</td>\n      <td>0.075780</td>\n      <td>0.054350</td>\n      <td>0.038250</td>\n      <td>0.064470</td>\n      <td>0.111750</td>\n      <td>0.142940</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>64.211750</td>\n      <td>2.655630</td>\n      <td>61.744950</td>\n      <td>63.997690</td>\n      <td>66.869360</td>\n      <td>6.542660</td>\n      <td>16.293150</td>\n      <td>3.114290</td>\n      <td>13.5190</td>\n      <td>15.6545</td>\n      <td>...</td>\n      <td>0.008470</td>\n      <td>0.010290</td>\n      <td>0.023100</td>\n      <td>0.066980</td>\n      <td>0.036650</td>\n      <td>0.051470</td>\n      <td>0.000000</td>\n      <td>0.027970</td>\n      <td>0.052820</td>\n      <td>0.080910</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>169</th>\n      <td>51.684594</td>\n      <td>2.950931</td>\n      <td>49.122250</td>\n      <td>51.338657</td>\n      <td>53.338589</td>\n      <td>7.694008</td>\n      <td>6.144695</td>\n      <td>1.181480</td>\n      <td>5.2240</td>\n      <td>5.9050</td>\n      <td>...</td>\n      <td>0.004002</td>\n      <td>0.010360</td>\n      <td>0.022415</td>\n      <td>0.032302</td>\n      <td>0.118365</td>\n      <td>0.061495</td>\n      <td>0.077229</td>\n      <td>0.106951</td>\n      <td>0.144127</td>\n      <td>0.136776</td>\n    </tr>\n    <tr>\n      <th>170</th>\n      <td>56.086409</td>\n      <td>5.981590</td>\n      <td>50.723267</td>\n      <td>56.433147</td>\n      <td>59.950764</td>\n      <td>16.290859</td>\n      <td>7.688258</td>\n      <td>2.892483</td>\n      <td>5.3490</td>\n      <td>7.2630</td>\n      <td>...</td>\n      <td>0.006362</td>\n      <td>0.009434</td>\n      <td>0.012003</td>\n      <td>0.017800</td>\n      <td>0.063056</td>\n      <td>0.048654</td>\n      <td>0.024658</td>\n      <td>0.057933</td>\n      <td>0.093155</td>\n      <td>0.128203</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>50.047239</td>\n      <td>2.401969</td>\n      <td>48.324600</td>\n      <td>49.654362</td>\n      <td>51.642471</td>\n      <td>6.221943</td>\n      <td>5.904786</td>\n      <td>1.073166</td>\n      <td>5.2730</td>\n      <td>5.6860</td>\n      <td>...</td>\n      <td>0.006031</td>\n      <td>0.014525</td>\n      <td>0.032093</td>\n      <td>0.042514</td>\n      <td>0.040851</td>\n      <td>0.040539</td>\n      <td>0.000000</td>\n      <td>0.036629</td>\n      <td>0.067197</td>\n      <td>0.097571</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>50.874526</td>\n      <td>2.089157</td>\n      <td>49.593983</td>\n      <td>50.240971</td>\n      <td>51.657730</td>\n      <td>4.288685</td>\n      <td>6.420369</td>\n      <td>0.943794</td>\n      <td>5.8540</td>\n      <td>6.2000</td>\n      <td>...</td>\n      <td>0.003055</td>\n      <td>0.005694</td>\n      <td>0.010639</td>\n      <td>0.031608</td>\n      <td>0.041429</td>\n      <td>0.038538</td>\n      <td>0.000000</td>\n      <td>0.039179</td>\n      <td>0.065948</td>\n      <td>0.091092</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>51.327230</td>\n      <td>1.785935</td>\n      <td>49.961189</td>\n      <td>51.132454</td>\n      <td>52.213837</td>\n      <td>3.964472</td>\n      <td>6.673368</td>\n      <td>0.785199</td>\n      <td>6.0770</td>\n      <td>6.5830</td>\n      <td>...</td>\n      <td>0.002669</td>\n      <td>0.004922</td>\n      <td>0.014560</td>\n      <td>0.026338</td>\n      <td>0.062299</td>\n      <td>0.050873</td>\n      <td>0.022004</td>\n      <td>0.057294</td>\n      <td>0.093313</td>\n      <td>0.130287</td>\n    </tr>\n  </tbody>\n</table>\n<p>174 rows × 36 columns</p>\n</div>"
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "data": {
      "text/plain": "     leq_mean   leq_std    leq_25  leq_median    leq_75  leq_10-leq_90  \\\n0    1.287807 -0.750280  1.318687    1.273593  1.076890      -0.685051   \n1    1.252559 -0.486631  1.330075    1.270557  1.145738      -0.469896   \n2    0.928563 -0.209032  0.851833    0.872181  0.841493      -0.181859   \n3    1.194090 -0.228639  1.205373    1.155453  1.097844      -0.168799   \n4    1.206778 -0.238520  1.066575    1.179239  1.310090      -0.263235   \n..        ...       ...       ...         ...       ...            ...   \n169 -1.025603 -0.099905 -0.910742   -0.990731 -1.123230      -0.060780   \n170 -0.241185  1.322692 -0.659946   -0.117450  0.065877       1.450910   \n171 -1.317386 -0.357588 -1.035692   -1.279448 -1.428253      -0.319631   \n172 -1.169960 -0.504423 -0.836846   -1.178893 -1.425509      -0.659579   \n173 -1.089287 -0.646756 -0.779324   -1.026078 -1.325501      -0.716590   \n\n     loudness_mean  loudness_std  loudness_25  loudness_median  ...  fluct_25  \\\n0         1.704452     -0.178628     1.738079         1.693258  ... -0.213462   \n1         1.276500     -0.075169     1.436683         1.417889  ... -0.262384   \n2         0.966664      0.210042     0.873872         0.955361  ... -0.308457   \n3         0.921273      0.168930     0.962755         0.935058  ...  2.121522   \n4         1.360967      0.687671     1.086979         1.312758  ... -0.183063   \n..             ...           ...          ...              ...  ...       ...   \n169      -1.023738     -0.657091    -0.887026        -0.962427  ... -0.395282   \n170      -0.661028      0.533348    -0.857279        -0.645518  ... -0.283182   \n171      -1.080113     -0.732451    -0.875365        -1.013534  ... -0.298899   \n172      -0.958960     -0.822462    -0.737102        -0.893585  ... -0.440254   \n173      -0.899509     -0.932806    -0.684033        -0.804206  ... -0.458617   \n\n     fluct_median  fluct_75  fluct_10-fluct_90  tonality_mean  tonality_std  \\\n0       -0.149548 -0.277698           0.238945      -0.037414     -0.103457   \n1       -0.279793 -0.383990           0.242630       0.420599     -0.215886   \n2       -0.346921  0.103662           0.487204      -0.519567     -0.542293   \n3        1.271380  1.015369          -0.140814       0.362035      0.355325   \n4       -0.268292 -0.205206           0.321762      -0.459324      0.250875   \n..            ...       ...                ...            ...           ...   \n169     -0.266412 -0.220433          -0.350824       1.255913      0.614457   \n170     -0.291190 -0.451975          -0.632095       0.094953      0.148759   \n171     -0.155040 -0.005236          -0.152756      -0.371153     -0.145581   \n172     -0.391213 -0.482297          -0.364293      -0.359018     -0.218124   \n173     -0.411850 -0.395106          -0.466489       0.079063      0.229217   \n\n     tonality_25  tonality_median  tonality_75  tonality_10-tonality_90  \n0       0.150891         0.048671    -0.065406                 0.028050  \n1       0.849978         0.434981     0.180441                -0.226995  \n2      -0.621828        -0.326215    -0.431462                -0.438777  \n3       0.425161         0.287946     0.411055                 0.462429  \n4      -0.621828        -0.484251    -0.477765                -0.365868  \n..           ...              ...          ...                      ...  \n169     1.492096         1.186687     0.899382                 0.380120  \n170     0.053114         0.149642     0.130588                 0.265646  \n171    -0.621828        -0.301060    -0.260915                -0.143394  \n172    -0.621828        -0.247106    -0.279766                -0.229908  \n173    -0.019541         0.136129     0.132982                 0.293473  \n\n[174 rows x 36 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>leq_mean</th>\n      <th>leq_std</th>\n      <th>leq_25</th>\n      <th>leq_median</th>\n      <th>leq_75</th>\n      <th>leq_10-leq_90</th>\n      <th>loudness_mean</th>\n      <th>loudness_std</th>\n      <th>loudness_25</th>\n      <th>loudness_median</th>\n      <th>...</th>\n      <th>fluct_25</th>\n      <th>fluct_median</th>\n      <th>fluct_75</th>\n      <th>fluct_10-fluct_90</th>\n      <th>tonality_mean</th>\n      <th>tonality_std</th>\n      <th>tonality_25</th>\n      <th>tonality_median</th>\n      <th>tonality_75</th>\n      <th>tonality_10-tonality_90</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.287807</td>\n      <td>-0.750280</td>\n      <td>1.318687</td>\n      <td>1.273593</td>\n      <td>1.076890</td>\n      <td>-0.685051</td>\n      <td>1.704452</td>\n      <td>-0.178628</td>\n      <td>1.738079</td>\n      <td>1.693258</td>\n      <td>...</td>\n      <td>-0.213462</td>\n      <td>-0.149548</td>\n      <td>-0.277698</td>\n      <td>0.238945</td>\n      <td>-0.037414</td>\n      <td>-0.103457</td>\n      <td>0.150891</td>\n      <td>0.048671</td>\n      <td>-0.065406</td>\n      <td>0.028050</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.252559</td>\n      <td>-0.486631</td>\n      <td>1.330075</td>\n      <td>1.270557</td>\n      <td>1.145738</td>\n      <td>-0.469896</td>\n      <td>1.276500</td>\n      <td>-0.075169</td>\n      <td>1.436683</td>\n      <td>1.417889</td>\n      <td>...</td>\n      <td>-0.262384</td>\n      <td>-0.279793</td>\n      <td>-0.383990</td>\n      <td>0.242630</td>\n      <td>0.420599</td>\n      <td>-0.215886</td>\n      <td>0.849978</td>\n      <td>0.434981</td>\n      <td>0.180441</td>\n      <td>-0.226995</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.928563</td>\n      <td>-0.209032</td>\n      <td>0.851833</td>\n      <td>0.872181</td>\n      <td>0.841493</td>\n      <td>-0.181859</td>\n      <td>0.966664</td>\n      <td>0.210042</td>\n      <td>0.873872</td>\n      <td>0.955361</td>\n      <td>...</td>\n      <td>-0.308457</td>\n      <td>-0.346921</td>\n      <td>0.103662</td>\n      <td>0.487204</td>\n      <td>-0.519567</td>\n      <td>-0.542293</td>\n      <td>-0.621828</td>\n      <td>-0.326215</td>\n      <td>-0.431462</td>\n      <td>-0.438777</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.194090</td>\n      <td>-0.228639</td>\n      <td>1.205373</td>\n      <td>1.155453</td>\n      <td>1.097844</td>\n      <td>-0.168799</td>\n      <td>0.921273</td>\n      <td>0.168930</td>\n      <td>0.962755</td>\n      <td>0.935058</td>\n      <td>...</td>\n      <td>2.121522</td>\n      <td>1.271380</td>\n      <td>1.015369</td>\n      <td>-0.140814</td>\n      <td>0.362035</td>\n      <td>0.355325</td>\n      <td>0.425161</td>\n      <td>0.287946</td>\n      <td>0.411055</td>\n      <td>0.462429</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.206778</td>\n      <td>-0.238520</td>\n      <td>1.066575</td>\n      <td>1.179239</td>\n      <td>1.310090</td>\n      <td>-0.263235</td>\n      <td>1.360967</td>\n      <td>0.687671</td>\n      <td>1.086979</td>\n      <td>1.312758</td>\n      <td>...</td>\n      <td>-0.183063</td>\n      <td>-0.268292</td>\n      <td>-0.205206</td>\n      <td>0.321762</td>\n      <td>-0.459324</td>\n      <td>0.250875</td>\n      <td>-0.621828</td>\n      <td>-0.484251</td>\n      <td>-0.477765</td>\n      <td>-0.365868</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>169</th>\n      <td>-1.025603</td>\n      <td>-0.099905</td>\n      <td>-0.910742</td>\n      <td>-0.990731</td>\n      <td>-1.123230</td>\n      <td>-0.060780</td>\n      <td>-1.023738</td>\n      <td>-0.657091</td>\n      <td>-0.887026</td>\n      <td>-0.962427</td>\n      <td>...</td>\n      <td>-0.395282</td>\n      <td>-0.266412</td>\n      <td>-0.220433</td>\n      <td>-0.350824</td>\n      <td>1.255913</td>\n      <td>0.614457</td>\n      <td>1.492096</td>\n      <td>1.186687</td>\n      <td>0.899382</td>\n      <td>0.380120</td>\n    </tr>\n    <tr>\n      <th>170</th>\n      <td>-0.241185</td>\n      <td>1.322692</td>\n      <td>-0.659946</td>\n      <td>-0.117450</td>\n      <td>0.065877</td>\n      <td>1.450910</td>\n      <td>-0.661028</td>\n      <td>0.533348</td>\n      <td>-0.857279</td>\n      <td>-0.645518</td>\n      <td>...</td>\n      <td>-0.283182</td>\n      <td>-0.291190</td>\n      <td>-0.451975</td>\n      <td>-0.632095</td>\n      <td>0.094953</td>\n      <td>0.148759</td>\n      <td>0.053114</td>\n      <td>0.149642</td>\n      <td>0.130588</td>\n      <td>0.265646</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>-1.317386</td>\n      <td>-0.357588</td>\n      <td>-1.035692</td>\n      <td>-1.279448</td>\n      <td>-1.428253</td>\n      <td>-0.319631</td>\n      <td>-1.080113</td>\n      <td>-0.732451</td>\n      <td>-0.875365</td>\n      <td>-1.013534</td>\n      <td>...</td>\n      <td>-0.298899</td>\n      <td>-0.155040</td>\n      <td>-0.005236</td>\n      <td>-0.152756</td>\n      <td>-0.371153</td>\n      <td>-0.145581</td>\n      <td>-0.621828</td>\n      <td>-0.301060</td>\n      <td>-0.260915</td>\n      <td>-0.143394</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>-1.169960</td>\n      <td>-0.504423</td>\n      <td>-0.836846</td>\n      <td>-1.178893</td>\n      <td>-1.425509</td>\n      <td>-0.659579</td>\n      <td>-0.958960</td>\n      <td>-0.822462</td>\n      <td>-0.737102</td>\n      <td>-0.893585</td>\n      <td>...</td>\n      <td>-0.440254</td>\n      <td>-0.391213</td>\n      <td>-0.482297</td>\n      <td>-0.364293</td>\n      <td>-0.359018</td>\n      <td>-0.218124</td>\n      <td>-0.621828</td>\n      <td>-0.247106</td>\n      <td>-0.279766</td>\n      <td>-0.229908</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>-1.089287</td>\n      <td>-0.646756</td>\n      <td>-0.779324</td>\n      <td>-1.026078</td>\n      <td>-1.325501</td>\n      <td>-0.716590</td>\n      <td>-0.899509</td>\n      <td>-0.932806</td>\n      <td>-0.684033</td>\n      <td>-0.804206</td>\n      <td>...</td>\n      <td>-0.458617</td>\n      <td>-0.411850</td>\n      <td>-0.395106</td>\n      <td>-0.466489</td>\n      <td>0.079063</td>\n      <td>0.229217</td>\n      <td>-0.019541</td>\n      <td>0.136129</td>\n      <td>0.132982</td>\n      <td>0.293473</td>\n    </tr>\n  </tbody>\n</table>\n<p>174 rows × 36 columns</p>\n</div>"
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess\n",
    "combined = combined.apply(lambda x: (x - x.mean()) / (x.std()))\n",
    "combined"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [],
   "source": [
    "# 转换为Tensor\n",
    "train_features = torch.tensor(combined[:num_of_train].values, dtype=torch.float)\n",
    "train_labels = torch.tensor(train_y.values, dtype=torch.float).view(-1, 1)\n",
    "test_features = torch.tensor(combined[num_of_train:].values, dtype=torch.float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([75, 36])"
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.3934,  1.0118, -0.7214,  ...,  0.7119,  0.6708,  0.9140],\n        [-1.1493,  0.2941, -1.2007,  ..., -0.0092,  0.0137,  0.1427],\n        [-0.3043, -0.2741, -0.1704,  ..., -0.2373, -0.2014, -0.0789],\n        ...,\n        [-1.3174, -0.3576, -1.0357,  ..., -0.3011, -0.2609, -0.1434],\n        [-1.1700, -0.5044, -0.8368,  ..., -0.2471, -0.2798, -0.2299],\n        [-1.0893, -0.6468, -0.7793,  ...,  0.1361,  0.1330,  0.2935]])"
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "def get_net():\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(train_features.shape[1], 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 1)\n",
    "    )\n",
    "    return net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [],
   "source": [
    "def get_rmse(net, features, labels):\n",
    "    rmse = torch.sqrt(loss(net(features), labels))\n",
    "    return rmse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#\n",
    "#     def __init__(self, features):\n",
    "#         super(Net, self).__init__()\n",
    "#\n",
    "#         self.linear_relu1 = nn.Linear(features, 128)\n",
    "#         self.linear_relu2 = nn.Linear(128, 256)\n",
    "#         self.linear_relu3 = nn.Linear(256, 256)\n",
    "#         self.linear_relu4 = nn.Linear(256, 256)\n",
    "#         self.linear5 = nn.Linear(256, 1)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         y_pred = self.linear_relu1(x)\n",
    "#         y_pred = nn.functional.relu(y_pred)\n",
    "#\n",
    "#         y_pred = self.linear_relu2(y_pred)\n",
    "#         y_pred = nn.functional.relu(y_pred)\n",
    "#\n",
    "#         y_pred = self.linear_relu3(y_pred)\n",
    "#         y_pred = nn.functional.relu(y_pred)\n",
    "#\n",
    "#         y_pred = self.linear_relu4(y_pred)\n",
    "#         y_pred = nn.functional.relu(y_pred)\n",
    "#\n",
    "#         y_pred = self.linear5(y_pred)\n",
    "#         return y_pred\n",
    "#\n",
    "# model = Net(features=train_features.shape[1])\n",
    "# # 使用MSE\n",
    "# criterion = nn.MSELoss(reduction='mean')\n",
    "# # 使用Adam优化器\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "def train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay,\n",
    "          batch_size):\n",
    "    train_ls, test_ls = [], []\n",
    "    train_iter = d2l.load_array((train_features, train_labels), batch_size)\n",
    "    # Adam\n",
    "    optimizer = torch.optim.Adam(net.parameters(),\n",
    "                                 lr=learning_rate,\n",
    "                                 weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            optimizer.zero_grad()\n",
    "            l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        train_ls.append(get_rmse(net, train_features, train_labels))\n",
    "        if test_labels is not None:\n",
    "            test_ls.append(get_rmse(net, test_features, test_labels))\n",
    "\n",
    "    return train_ls, test_ls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [],
   "source": [
    "def get_k_fold_data(k, i, X, y):\n",
    "    assert k > 1\n",
    "    fold_size = X.shape[0] // k\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = torch.cat([X_train, X_part], 0)\n",
    "            y_train = torch.cat([y_train, y_part], 0)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [],
   "source": [
    "def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay, batch_size):\n",
    "    train_l_sum, valid_l_sum = 0, 0\n",
    "    for i in range(k):\n",
    "        data = get_k_fold_data(k, i, X_train, y_train)\n",
    "        net = get_net()\n",
    "        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate, weight_decay, batch_size)\n",
    "        train_l_sum += train_ls[-1]\n",
    "        valid_l_sum += valid_ls[-1]\n",
    "        # if i == 0:\n",
    "        #     d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls], xlabel='epoch', ylabel='rmse',\n",
    "        #              xlim=[1, num_epochs], legend=['train', 'valid'], yscale='linear')\n",
    "        print(f'折{i + 1}，训练 rmse{float(train_ls[-1]):f}, 'f'验证 rmse{float(valid_ls[-1]):f}')\n",
    "\n",
    "    return train_l_sum / k, valid_l_sum / k"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折1，训练 rmse858921.437500, 验证 rmse883900.687500\n",
      "折2，训练 rmse8.717760, 验证 rmse8.250883\n",
      "折3，训练 rmse555730.750000, 验证 rmse616918016.000000\n",
      "折4，训练 rmse584.797485, 验证 rmse627.345154\n",
      "折5，训练 rmse2761.628418, 验证 rmse203506597888.000000\n",
      "5-折验证: 平均训练 rmse: 283601.437500, 平均验证 rmse: 40824881152.000000\n"
     ]
    }
   ],
   "source": [
    "# 参数选择\n",
    "k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64\n",
    "train_l, valid_l = k_fold(k, train_features,\n",
    "                          train_labels, num_epochs, lr, weight_decay, batch_size)\n",
    "print(f'{k}-折验证: 平均训练 rmse: {float(train_l):f}, '\n",
    "      f'平均验证 rmse: {float(valid_l):f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [],
   "source": [
    "def train_and_pred(train_features, test_features, train_labels, test_data, num_epochs, lr, weight_decay, batch_size):\n",
    "    net = get_net()\n",
    "    train_ls, _ = train(net, train_features, train_labels, None, None, num_epochs, lr, weight_decay, batch_size)\n",
    "    print(f'训练log rmse：{float(train_ls[-1]):f}')\n",
    "\n",
    "    # 将网络用于测试集\n",
    "    preds = net(test_features).detach().numpy()\n",
    "    return preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练log rmse：32782.167969\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [43964.18  ],\n       [ 1592.6978],\n       [ 1592.6978],\n       [43964.18  ],\n       [43964.18  ],\n       [ 1592.6978],\n       [43964.18  ],\n       [ 1592.6978],\n       [ 1592.6978],\n       [43964.18  ],\n       [43964.18  ],\n       [43964.18  ],\n       [43964.18  ],\n       [43964.18  ],\n       [43964.18  ],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978],\n       [ 1592.6978]], dtype=float32)"
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = train_and_pred(train_features, test_features, train_labels, test, num_epochs, lr, weight_decay, batch_size)\n",
    "preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}